import argparse
import json
import multiprocessing
import os
import os.path as osp
import shutil
import sys
import time
from datetime import datetime

from aphrodite_coin.blockchain import validate_transaction, process_blockchain_data
from aphrodite_coin.visualizer import generate_token_visualization, apply_theme
from aphrodite_coin.analytics import analyze_data, check_data_integrity

NUM_ITERATIONS = 3


def print_time():
    """Log the current timestamp."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] - Log timestamp")


def parse_arguments():
    """Parse command-line arguments for the Aphrodite AI Coin pipeline."""
    parser = argparse.ArgumentParser(description="Aphrodite AI Coin: Blockchain Data Processing and Visualization")
    parser.add_argument("--data", type=str, required=True, help="Path to input blockchain data")
    parser.add_argument("--theme", type=str, default="divine",
                        choices=["divine", "ethereal", "minimalist"],
                        help="Visualization theme for token analytics")
    parser.add_argument("--output", type=str, default="output", help="Directory for processed outputs")
    parser.add_argument("--iterations", type=int, default=NUM_ITERATIONS, help="Number of iterations for refinement")
    parser.add_argument("--parallel", type=int, default=0, help="Number of parallel processes (0 for sequential)")
    return parser.parse_args()


def worker(task_queue, theme, output_dir, lock):
    """Process tasks for blockchain visualization."""
    print(f"[Worker] Starting worker process.")
    while True:
        task = task_queue.get()
        if task is None:  # Termination signal
            break
        with lock:
            print(f"[Worker] Processing task: {task['name']}")
            try:
                data = process_blockchain_data(task["data"])
                if not check_data_integrity(data):
                    raise ValueError(f"Data integrity check failed for task {task['name']}.")
                token_visualization = generate_token_visualization(data, theme=theme)
                apply_theme(token_visualization, theme)
                save_token_visualization(token_visualization, output_dir, task["name"])
                print(f"[Worker] Task {task['name']} completed successfully.")
            except Exception as e:
                print(f"[Error] Task {task['name']} failed: {e}")
    print("[Worker] Worker process finished.")


def save_token_visualization(token_visualization, output_dir, task_name):
    """Save the token visualization to the output directory."""
    os.makedirs(output_dir, exist_ok=True)
    output_path = osp.join(output_dir, f"{task_name}_token_visualization.png")
    token_visualization.save(output_path)
    print(f"[Output] Saved token visualization to {output_path}")


if __name__ == "__main__":
    args = parse_arguments()

    print_time()
    print(f"Starting Aphrodite AI Coin Pipeline with theme: {args.theme}")

    # Prepare output directory
    os.makedirs(args.output, exist_ok=True)

    # Prepare tasks
    tasks = [{"data": args.data, "name": f"task_{i}"} for i in range(args.iterations)]

    if args.parallel > 0:
        # Parallel processing
        task_queue = multiprocessing.Queue()
        lock = multiprocessing.Lock()

        # Add tasks to queue
        for task in tasks:
            task_queue.put(task)

        # Start workers
        workers = []
        for i in range(args.parallel):
            worker_process = multiprocessing.Process(
                target=worker,
                args=(task_queue, args.theme, args.output, lock)
            )
            worker_process.start()
            workers.append(worker_process)

        # Add termination signals to queue
        for _ in range(args.parallel):
            task_queue.put(None)

        # Wait for workers to finish
        for worker_process in workers:
            worker_process.join()
    else:
        # Sequential processing
        for task in tasks:
            try:
                print(f"[Sequential] Processing task: {task['name']}")
                data = process_blockchain_data(task["data"])
                if not check_data_integrity(data):
                    raise ValueError(f"Data integrity check failed for task {task['name']}.")
                token_visualization = generate_token_visualization(data, theme=args.theme)
                apply_theme(token_visualization, args.theme)
                save_token_visualization(token_visualization, args.output, task["name"])
                print(f"[Sequential] Task {task['name']} completed successfully.")
            except Exception as e:
                print(f"[Error] Task {task['name']} failed: {e}")

    print(f"All tasks completed. Results are stored in: {args.output}")
    print_time()

